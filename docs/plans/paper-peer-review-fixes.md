# Plan: Address Peer Review Feedback for SimCash Paper

**Date:** 2025-12-21
**Status:** Draft
**Priority:** High

## Overview

This plan addresses 13 issues raised in peer review of the SimCash paper generated by `docs/papers/simcash-paper/paper_generator`. The reviewer identified terminology problems, inconsistencies between claims and results, and documentation mismatches.

---

## Implementation Analysis: What The Code Actually Does

Before fixing the paper, I analyzed the actual implementation to understand what the code does vs. what the paper claims.

### Convergence Detection (from `api/payment_simulator/ai_cash_mgmt/optimization/convergence_detector.py`)

**Deterministic-Temporal Mode (Experiments 1 & 3):**
- Policies are **ALWAYS accepted** (line 2650 of `optimization.py`)
- Convergence based on **policy parameter stability** (`initial_liquidity_fraction`), NOT cost stability
- Requires ALL agents' `initial_liquidity_fraction` unchanged for `stability_window` iterations (default: 5)
- Multi-agent convergence: ALL optimized agents must have stable policies simultaneously (lines 2915-2927)

**Bootstrap Mode (Experiment 2):**
- Three criteria must ALL be satisfied (lines 488-490):
  1. **CV < 3%**: Coefficient of variation over last 5 iteration means
  2. **No trend**: Mann-Kendall test p > 0.05
  3. **Regret < 10%**: Current cost within 10% of best observed

### Acceptance Rule (from `api/payment_simulator/experiments/runner/optimization.py`)

**Bootstrap mode** (lines 2831-2834):
```python
# Accept if delta_sum > 0 (new policy is cheaper overall)
should_accept = evaluation.delta_sum > 0
```
- `delta_sum = sum(old_cost - new_cost)` across all bootstrap samples
- This is **sum of improvements**, NOT "improvement on all samples"

**Deterministic-temporal mode** (line 2650):
- Policies are **always accepted** - no cost-based rejection

### Agent Update Order (from `optimization.py` lines 886-888)

```python
for agent_id in self.optimized_agents:
    await self._optimize_agent(agent_id, ...)
```
- **Sequential within iteration**: Agent A optimized first, then Agent B
- Agent B sees Agent A's **updated policy from same iteration**
- This is NOT simultaneous - creates first-mover dynamics

---

## Summary of Issues

| # | Issue | Severity | Files Affected |
|---|-------|----------|----------------|
| 1 | "Nash equilibrium" too strong | üî¥ Critical | abstract.py, methods.py, discussion.py, conclusion.py |
| 2 | Convergence criterion doesn't match tables | üî¥ Critical | methods.py |
| 3 | Bootstrap acceptance rule mismatch | üü° Medium | methods.py |
| 4 | Sequential vs simultaneous ambiguity | üü° Medium | methods.py |
| 5 | "Successful recovery" claim inaccurate | üü° Medium | introduction.py |
| 6 | Free-rider interpretation issues | üü° Medium | discussion.py, results.py |
| 7 | Stochastic environment looks deterministic | üü° Medium | discussion.py |
| 8 | CV/Mann-Kendall clarification | üü¢ Minor | methods.py |
| 9 | Numbers/summary table inconsistencies | üü¢ Minor | discussion.py |
| 10 | Sample size vs language strength | üü° Medium | abstract.py, conclusion.py |
| 11 | RL vs LLM sample efficiency | üü¢ Minor | discussion.py |
| 12 | Authorship/LLM details | üü¢ Minor | (paper_builder.py - boilerplate) |
| 13 | Minor technical nits | üü¢ Minor | methods.py, appendices.py |

---

## Detailed Fix Plan

### Issue 1: "Nash equilibrium" terminology is too strong üî¥

**Problem:** The paper claims outcomes are "Nash equilibria" and "mutual best-response equilibria," but:
- Convergence is defined as "liquidity unchanged for 5 iterations," not "no unilateral deviation reduces cost"
- Policies are unconditionally accepted, even if they increase cost
- No formal Nash verification is performed

**Reviewer recommendation:** Soften to "equilibrium-like fixed points" or "fixed points of the update rule"

**Files to modify:**

#### `src/sections/abstract.py`
- Line 36: Change "Can Large Language Models discover Nash equilibria" ‚Üí "Can Large Language Models discover equilibrium-like behavior"
- Line 46: Change "reliably converge to stable equilibria" ‚Üí "reliably converge to stable fixed points"
- Line 48: Change "asymmetric free-rider equilibria" ‚Üí "asymmetric free-rider outcomes"
- Line 52-53: Add explicit caveat about not formally verifying Nash condition

#### `src/sections/methods.py`
- Line 123: Change "identify Nash equilibria" ‚Üí "identify stable policy profiles"
- Line 129: Change "indicating mutual best-response equilibrium" ‚Üí "indicating policy stability (treated as equilibrium-like behavior)"
- Add new paragraph in Section 2.5 explicitly stating: "We treat stable policy pairs as 'equilibria of the learning dynamics' but do not formally verify the Nash condition (i.e., we do not check that no unilateral deviation would reduce cost)."

#### `src/sections/discussion.py`
- Line 87: Change "converge to stable equilibria" ‚Üí "converge to stable policy profiles"
- Line 93: Change "observed equilibria" ‚Üí "observed outcomes"
- Line 109: Change "multiple asymmetric equilibria" ‚Üí "multiple asymmetric stable outcomes"
- Line 160: Change "symmetric games can support asymmetric equilibria" ‚Üí "symmetric games can converge to asymmetric stable outcomes under sequential optimization"

#### `src/sections/conclusion.py`
- Line 34: Change "discovering Nash equilibria" ‚Üí "discovering equilibrium-like behavior"
- Lines 45-46: Change "asymmetric free-rider equilibria" ‚Üí "asymmetric free-rider outcomes"
- Line 71: Change "select among multiple equilibria" ‚Üí "select among multiple stable outcomes"

#### `src/sections/results.py`
- Line 201: Change "asymmetric equilibrium" ‚Üí "asymmetric stable outcome"
- Line 216-218: Reframe discussion of "multiple equilibria" to "multiple stable outcomes"
- Line 282: Change "asymmetric equilibria" ‚Üí "asymmetric outcomes"

---

### Issue 2: Convergence criterion doesn't match tables üî¥

**Problem:** Section 2.5.1 says "Both agents' initial_liquidity_fraction unchanged for 5 consecutive iterations," but tables show:
- Exp 1 Pass 1: Stability only from iterations 5-8 (4 iterations)
- Exp 1 Pass 2: BANK_B liquidity wiggles until final iteration
- Exp 3: "Equilibrium" values repeat for only 2-4 iterations

**Root cause (from code analysis):**

The paper description is **partially wrong**. The actual implementation in `optimization.py` (lines 2900-2927):

1. **Policy stability check** uses `_is_stable_change()` which checks if relative change ‚â§ `stability_threshold` (default 5%)
2. This means "unchanged" really means "changed by ‚â§ 5%"
3. Multi-agent convergence requires ALL agents stable for `stability_window` (default 5) iterations

**The paper says "unchanged" but implementation allows ‚â§5% change.**

**Fix:**

#### `src/sections/methods.py`
- Line 129: Change from:
  > "Convergence criterion: Both agents' initial_liquidity_fraction unchanged for 5 consecutive iterations"

  To:
  > "Convergence criterion: Both agents' initial_liquidity_fraction stable (relative change ‚â§ 5%) for 5 consecutive iterations"

- Also clarify that in deterministic-temporal mode, **all policies are accepted** regardless of cost (the convergence is about when the LLM stops exploring, not cost improvement).

---

### Issue 3: Bootstrap acceptance rule text/math mismatch üü°

**Problem:** Section 2.5.2 says:
> "Accept if ‚àë·µ¢ Œ¥·µ¢ > 0 (improvement across all samples)"

But mathematically, ‚àë Œ¥·µ¢ > 0 means **positive average improvement**, not "improvement for all samples."

**Fix:**

#### `src/sections/methods.py`
- Line 140: Change "(improvement across all samples)" ‚Üí "(positive mean improvement across bootstrap samples)"
- Or more explicitly: "(i.e., average improvement across bootstrap samples)"

---

### Issue 4: Sequential vs simultaneous ambiguity üü°

**Problem:** Two conflicting statements:
- Section 2.5.1: "counterparty policies evolve simultaneously"
- Section 2.7: "we optimize agents sequentially within each iteration"

**Root cause (from code analysis):**

The implementation is **SEQUENTIAL** (not simultaneous). From `optimization.py` lines 886-888:
```python
for agent_id in self.optimized_agents:
    await self._optimize_agent(agent_id, ...)
```

Agent B sees Agent A's **updated** policy from the same iteration. This creates asymmetric first-mover dynamics.

**Fix:**

#### `src/sections/methods.py`
- Line 128: REMOVE the word "simultaneously" - it's wrong
- Change from:
  > "Rationale: Cost-based rejection would cause oscillation in multi-agent settings where counterparty policies evolve simultaneously"

  To:
  > "Rationale: Cost-based rejection in deterministic settings would cause oscillation, as the opponent's policy also changes each iteration"

- Line 183: Keep "we optimize agents sequentially" - this is CORRECT
- Add explicit clarification near line 183:
  > "Within each iteration, BANK_A is optimized first (holding BANK_B's previous policy fixed), then BANK_B is optimized given BANK_A's newly updated policy. This sequential structure creates first-mover dynamics where the agent order can influence equilibrium selection."

---

### Issue 5: "Successful recovery" claim doesn't match results üü°

**Problem:** Contribution line 2 says:
> "Empirical Validation: Successful recovery of Castro et al.'s theoretical equilibria"

But:
- Experiment 2 only partially aligns (BANK_A ends ~5%, below 10-30% band)
- Experiment 3 explicitly does NOT recover symmetric predicted equilibrium

**Fix:**

#### `src/sections/introduction.py`
- Line 43-44: Change "Successful recovery of Castro et al.'s theoretical equilibria" ‚Üí "Empirical comparison with Castro et al.'s theoretical predictions, showing partial alignment and systematic deviations"

---

### Issue 6: Free-rider interpretation and multiple equilibria üü°

**Problem 1:** In Exp 1 Pass 3, the "free-rider" (BANK_B at 0%) has HIGHER cost ($70) than BANK_A ($31.78). Free-riders typically *benefit* by contributing less.

**Problem 2:** Inferring "the game has multiple equilibria" from different algorithm endpoints doesn't establish multiple Nash equilibria‚Äîjust multiple attractors.

**Fix:**

#### `src/sections/discussion.py`
- Lines 106-111: Clarify that Pass 3 represents a **failed coordination** rather than a true free-rider equilibrium
- Reframe: "BANK_B's zero-liquidity strategy in Pass 3 led to high costs for both agents, representing a coordination failure rather than successful free-riding"
- Lines 109-110: Change "game admits multiple asymmetric equilibria" ‚Üí "learning dynamics can converge to multiple asymmetric stable points with different efficiency properties"

#### `src/sections/results.py`
- Lines 212-218: Reframe Pass 3 as "coordination failure" not "alternative equilibrium"
- Add: "Note that BANK_B's zero-liquidity outcome in Pass 3, while stable, resulted in higher costs than Passes 1-2, representing a suboptimal fixed point rather than a true free-rider success."

---

### Issue 7: "Stochastic" environment looks nearly deterministic üü°

**Problem:** Experiment 2 iteration tables look almost perfectly smooth (near-linear cost decrease), despite being described as stochastic (Poisson arrivals, lognormal amounts).

**Explanation needed:** Context simulation reuses a fixed transaction schedule per iteration, so costs are deterministic functions of liquidity within each iteration. Stochasticity enters via bootstrap resampling.

**Fix:**

#### `src/sections/discussion.py` (or results.py)
- Add explicit explanation in Experiment 2 section:
  > "For the context simulation we hold the transaction path fixed (one sampled day), so costs are deterministic functions of liquidity; stochasticity enters only via the bootstrap resampling used for evaluation."

#### `src/sections/results.py`
- Lines 238-245: Strengthen the explanation of context simulation vs bootstrap

---

### Issue 8: CV and Mann-Kendall clarification üü¢

**Problem 1:** Mann-Kendall test with only 5 iterations has very low power; p-values are coarse.

**Problem 2:** Table 5 shows very large CVs across bootstrap samples (128%), but convergence uses CV < 3%. Need to clarify these are different CVs.

**Fix:**

#### `src/sections/methods.py`
- Lines 141-146: Add clarification:
  > "Note: The Mann-Kendall test is a heuristic trend test over a short horizon; with only 5 iterations, statistical power is limited."
  > "The CV < 3% threshold is computed over the last 5 iteration means, not over individual bootstrap samples within an iteration (which can exhibit much higher variance, as shown in Table 5)."

---

### Issue 9: Numbers/summary table inconsistencies üü¢

**Problem:** Table 4.1.4 says Exp 2 "Observed: 6-20%" but actual reported values are 5-12%.

**Fix:**

#### `src/sections/discussion.py`
- Lines 173: Update summary table to show accurate observed range "5-12%" not "6-20%"

**Also:**
- Line 47-48 (conclusion): Nuance the "one agent minimizes while other compensates" claim:
  > "typically one agent settles on a very low liquidity fraction while the other maintains a higher fraction; even when both agents' costs are high (Exp 1 Pass 3), the outcomes remain asymmetric"

---

### Issue 10: Sample size vs strength of language üü°

**Problem:** Claims sound stronger than 9 preliminary runs support.

**Fixes:**

#### `src/sections/abstract.py`
- Line 46: Add "in our 9 preliminary runs" qualifier

#### `src/sections/conclusion.py`
- Line 45: Change "Asymmetric equilibria dominate" ‚Üí "Asymmetric outcomes dominate in our experiments"

#### `src/sections/discussion.py`
- Line 160: Change "This finding suggests that symmetric games can support asymmetric equilibria" ‚Üí "This finding suggests that, under our sequential LLM update rule, symmetric games tend to converge to asymmetric outcomes"

---

### Issue 11: RL vs LLM sample efficiency comparison üü¢

**Problem:** Paper contrasts "50-100 RL episodes" with "7-29 LLM iterations," but bootstrap mode uses 50 samples √ó 2 policies = 100 simulations per iteration, so total simulations are comparable.

**Fix:**

#### `src/sections/discussion.py`
- Lines 206-208: Clarify comparison is about **decision steps** (iterations requiring LLM reasoning), not raw simulation count
- Add: "While each bootstrap iteration involves ~100 simulation samples for evaluation, the number of LLM decision points (policy proposals requiring reasoning) remains 7-29, compared to the continuous gradient updates in RL training."

---

### Issue 12: Authorship/LLM details üü¢

**Problem:** Listing Claude as "co-author" may distract collaborators; many venues disallow LLM authorship.

**Fix:**

#### Paper metadata (paper_builder.py or templates)
- Change authorship section to: "Developed with assistance from Claude 4.5 and GPT-5.2 as coding and writing tools; all experimental design and interpretation are the author's own."
- Keep detailed model configuration in Methods section

---

### Issue 13: Minor technical nits üü¢

**Problem 1:** Section 2.2 cost function list omits overdraft cost, but Section 2.4.1 includes it.

**Problem 2:** Appendix headings say "Symmetric Equilibrium ‚Äì Detailed Results" but observed outcomes are asymmetric.

**Problem 3:** Mixed decimal precision in tables.

**Fixes:**

#### `src/sections/methods.py`
- Line 39-45: Add overdraft cost to the cost function list:
  ```
  \item \textbf{Overdraft cost}: Fee for negative balance (basis points per day)
  ```

#### `src/sections/appendices.py`
- Rename "Symmetric Equilibrium ‚Äì Detailed Results" ‚Üí "Symmetric Scenario ‚Äì Detailed Results"

#### `src/latex/formatting.py`
- Harmonize decimal precision in format_percent() and format_money()

---

## Implementation Order

**Phase 1: Critical terminology fixes (Issues 1, 2, 4)** ‚è±Ô∏è ~2 hours
1. Systematically replace "Nash equilibrium/equilibria" with "stable fixed point" / "equilibrium-like behavior"
2. Fix convergence criterion: "unchanged" ‚Üí "stable (relative change ‚â§ 5%)"
3. Fix "simultaneously" ‚Üí sequential update clarification
4. Regenerate paper and verify changes

**Phase 2: Accuracy fixes (Issues 3, 5, 6)** ‚è±Ô∏è ~1 hour
1. Fix bootstrap acceptance rule: "improvement across all samples" ‚Üí "positive sum of improvements"
2. Downgrade "successful recovery" ‚Üí "empirical comparison showing partial alignment"
3. Reframe free-rider interpretation (Pass 3 = coordination failure, not free-rider success)

**Phase 3: Clarity improvements (Issues 7, 8, 9, 10, 11)** ‚è±Ô∏è ~1 hour
1. Add explanation: context simulation uses fixed transaction path; stochasticity via bootstrap
2. Clarify CV: computed over iteration means, not individual samples
3. Fix Exp 2 observed range: "6-20%" ‚Üí "5-12%"
4. Add sample size qualifiers throughout
5. Clarify RL comparison: decision steps vs simulation count

**Phase 4: Polish (Issues 12, 13)** ‚è±Ô∏è ~30 mins
1. Update authorship to "assisted by" rather than "co-author"
2. Add overdraft cost to Section 2.2 cost function list
3. Rename appendix: "Symmetric Equilibrium" ‚Üí "Symmetric Scenario"
4. Harmonize decimal precision in formatting.py

---

## Testing Plan

After implementing fixes:

1. **Regenerate paper:**
   ```bash
   cd docs/papers/simcash-paper/paper_generator
   ./generate_paper.sh
   ```

2. **Verify terminology changes:**
   ```bash
   grep -n "Nash equilib" output/paper.tex  # Should be minimal/absent
   grep -n "stable.*point\|fixed point\|equilibrium-like" output/paper.tex  # Should appear
   ```

3. **Verify convergence criterion matches implementation:**
   - Check actual experiment runner code
   - Ensure methods.py matches

4. **Manual review:**
   - Read generated abstract, conclusion, discussion for consistency
   - Verify numerical claims match tables

5. **PDF compilation:**
   - Compile to PDF and visually inspect
   - Verify no LaTeX errors

---

## Open Questions for User

Before implementing, please confirm:

1. **Convergence criterion:** What is the actual implemented rule? Is it:
   - Exact match for 5 iterations?
   - Within some tolerance Œµ?
   - Different window size?

2. **Authorship preference:** Do you want to:
   - Remove Claude from author list entirely?
   - Keep as acknowledgment only?
   - Keep current phrasing?

3. **Priority:** Should I implement all fixes, or focus on critical/medium severity first?

---

## Files Modified Summary

| File | Changes |
|------|---------|
| `src/sections/abstract.py` | Lines 36, 46-48, 52-53: Soften equilibrium language |
| `src/sections/introduction.py` | Lines 43-44: Downgrade "successful recovery" |
| `src/sections/methods.py` | Lines 39-45, 123, 128-129, 140-147, 183: Fix convergence, acceptance, sequential/simultaneous |
| `src/sections/discussion.py` | Lines 87, 93, 109, 117-140, 160, 173, 206-208: Multiple fixes |
| `src/sections/results.py` | Lines 201, 212-218, 238-245, 282, 305-306: Terminology, explanation |
| `src/sections/conclusion.py` | Lines 34, 45-46, 71: Soften claims |
| `src/sections/appendices.py` | Heading rename |
| `src/latex/formatting.py` | Decimal precision harmonization |
| Paper metadata | Authorship clarification |

---

---

## Appendix: Specific Code Changes

### File: `src/sections/abstract.py`

```python
# Line 36: Soften opening question
# OLD: "Can Large Language Models discover Nash equilibria through strategic reasoning alone?"
# NEW: "Can Large Language Models discover equilibrium-like behavior through strategic reasoning alone?"

# Line 46: Soften convergence claim
# OLD: "reliably converge to stable equilibria"
# NEW: "reliably converge to stable policy profiles"

# Line 46: Add qualifier
# OLD: "({convergence_pct}\% success, mean {avg_iterations:.1f} iterations)"
# NEW: "({convergence_pct}\% success in our {total_passes} preliminary runs, mean {avg_iterations:.1f} iterations)"

# Lines 48-49: Soften equilibrium language
# OLD: "asymmetric free-rider equilibria"
# NEW: "asymmetric free-rider outcomes"

# Lines 52-53: Add caveat
# OLD: "equilibrium behavior without explicit game-theoretic modeling"
# NEW: "equilibrium-like behavior without explicit game-theoretic modeling (though we do not formally verify the Nash condition)"
```

### File: `src/sections/introduction.py`

```python
# Lines 43-44: Downgrade contribution claim
# OLD: "\item \textbf{Empirical Validation}: Successful recovery of Castro et al.'s theoretical equilibria"
# NEW: "\item \textbf{Empirical Comparison}: Comparison with Castro et al.'s theoretical predictions, showing partial alignment and systematic deviations"
```

### File: `src/sections/methods.py`

```python
# Lines 39-45: Add overdraft cost to list
# ADD after "Deadline penalty" item:
#     \item \textbf{Overdraft cost}: Fee for negative balance (basis points per day)

# Line 123: Soften Nash language
# OLD: "we use \textbf{temporal policy stability} to identify Nash equilibria"
# NEW: "we use \textbf{temporal policy stability} to identify stable policy profiles"

# Line 127: Clarify unconditional acceptance
# ADD after "Unconditional acceptance" item:
#     Note: In this mode, ALL LLM-proposed policies are accepted regardless of cost impact

# Line 128: Fix "simultaneously" error
# OLD: "counterparty policies evolve simultaneously"
# NEW: "the opponent's policy also changes each iteration"

# Line 129: Fix convergence criterion
# OLD: "Both agents' initial_liquidity_fraction unchanged for 5 consecutive iterations, indicating mutual best-response equilibrium"
# NEW: "Both agents' initial_liquidity_fraction stable (relative change $\leq$ 5\%) for 5 consecutive iterations, indicating policy stability"

# Line 140: Fix acceptance rule description
# OLD: "(improvement across all samples)"
# NEW: "(positive sum of improvements across all samples)"

# Lines 141-146: Add CV clarification
# ADD note: "The CV threshold is computed over the last 5 \textit{iteration means}, not over individual bootstrap samples within an iteration."

# After line 183: Add sequential dynamics explanation
# ADD: "Within each iteration, BANK\_A is optimized first (holding BANK\_B's previous policy fixed), then BANK\_B is optimized given BANK\_A's newly updated policy. This sequential structure can influence which stable outcome is selected."
```

### File: `src/sections/discussion.py`

```python
# Line 87: Soften equilibrium language
# OLD: "converge to stable equilibria"
# NEW: "converge to stable policy profiles"

# Line 93: Soften equilibrium language
# OLD: "observed equilibria"
# NEW: "observed outcomes"

# Lines 106-111: Reframe Pass 3 as coordination failure
# ADD after "role reversal" description:
#     Note that BANK\_B's zero-liquidity outcome, while stable, resulted in \textit{higher}
#     costs than the compensating agent---a coordination failure rather than successful free-riding.

# Lines 109-110: Soften multiple equilibria claim
# OLD: "the game admits \textbf{multiple asymmetric equilibria}"
# NEW: "the learning dynamics can converge to \textbf{multiple asymmetric stable outcomes}"

# Line 117-140: Add stochastic explanation
# ADD after bootstrap stats discussion:
#     \textbf{Methodological note on smoothness:} The iteration-by-iteration costs appear nearly
#     deterministic because the context simulation holds the transaction path fixed (one sampled day);
#     stochasticity enters only via bootstrap resampling used for policy evaluation.

# Line 160: Soften generalization
# OLD: "symmetric games can support asymmetric equilibria"
# NEW: "under our sequential LLM update rule, symmetric games tend to converge to asymmetric stable outcomes"

# Line 173: Fix observed range
# OLD: "Observed: 6--20\%"
# NEW: "Observed: 5--12\%"

# Lines 206-208: Clarify sample efficiency comparison
# CHANGE "7--29 iterations" explanation to add:
#     While each bootstrap iteration involves $\sim$100 simulation samples for evaluation,
#     the number of LLM decision points (policy proposals requiring reasoning) remains 7--29.
```

### File: `src/sections/results.py`

```python
# Line 201: Soften equilibrium language
# OLD: "asymmetric equilibrium"
# NEW: "asymmetric stable outcome"

# Lines 212-218: Reframe Pass 3
# OLD: "the game admits multiple equilibria"
# NEW: "the learning dynamics can converge to multiple stable outcomes"

# ADD after line 218:
#     Note that the Pass 3 outcome, while stable, is \textit{not} Pareto-optimal---both agents
#     would prefer the efficient outcome from Passes 1--2. This represents a coordination failure
#     of the learning dynamics rather than a true free-rider success.

# Line 282: Soften equilibrium language
# OLD: "asymmetric equilibria"
# NEW: "asymmetric outcomes"

# Lines 305-306: Soften equilibrium language
# OLD: "asymmetric equilibria with free-rider behavior"
# NEW: "asymmetric stable outcomes with free-rider patterns"
```

### File: `src/sections/conclusion.py`

```python
# Line 34: Soften main claim
# OLD: "discovering Nash equilibria"
# NEW: "discovering equilibrium-like behavior"

# Lines 45-46: Soften and qualify
# OLD: "Asymmetric equilibria dominate"
# NEW: "Asymmetric outcomes dominate in our experiments"

# Lines 47-48: Nuance the compensation claim
# OLD: "One agent minimizes liquidity while the other compensates---a pattern that emerged in all nine passes"
# NEW: "Typically one agent settles on very low liquidity while the other maintains higher allocation; even in suboptimal outcomes (Exp 1 Pass 3), the results remain asymmetric"

# Line 71: Soften equilibrium selection claim
# OLD: "select among multiple equilibria"
# NEW: "select among multiple stable outcomes"
```

### File: `src/sections/appendices.py`

```python
# Find and replace heading
# OLD: "Symmetric Equilibrium ‚Äì Detailed Results"
# NEW: "Symmetric Scenario ‚Äì Detailed Results"
```

### File: Paper metadata (paper_builder.py or templates)

```python
# Find authorship section and change:
# OLD: "developed in collaboration with Claude 4.5 Opus (Anthropic), which served as a co-author"
# NEW: "developed with assistance from Claude 4.5 Opus (Anthropic) and GPT-5.2 (OpenAI) as coding and writing tools; all experimental design, analysis, and interpretation are the author's own"
```

---

## Terminology Replacement Summary

| Old Term | New Term | Rationale |
|----------|----------|-----------|
| Nash equilibrium/equilibria | stable policy profile / equilibrium-like behavior | Not formally verified as Nash |
| mutual best-response equilibrium | policy stability | Describes learning dynamics, not game theory |
| discover equilibria | discover equilibrium-like behavior | Softer claim |
| game admits multiple equilibria | learning dynamics can converge to multiple stable outcomes | About algorithm, not game |
| free-rider equilibrium | free-rider outcome / pattern | Outcome may not be true equilibrium |
| unchanged for 5 iterations | stable (‚â§5% change) for 5 iterations | Matches actual implementation |
| improvement across all samples | positive sum of improvements | Matches actual delta_sum criterion |
| evolve simultaneously | evolve each iteration | Sequential, not simultaneous |

---

*Plan created: 2025-12-21*
