# Castro Experiment Replay Identity

**Version**: 1.0
**Created**: 2025-12-09
**Status**: PLANNED

---

## Overview

Implement replay identity for Castro experiments so that:
1. `uv run castro replay <RUN_ID>` produces **byte-for-byte identical** verbose output as the original run
2. `uv run castro results` lists all experiment runs with IDs, dates, and metrics
3. Experiment run ID is printed in experiment output

This follows the StateProvider pattern from the Payment Simulator (`docs/reference/api/state-provider.md`) to ensure replay identity is maintainable as new features are added.

---

## Problem Statement

### Current State

Currently, Castro experiment verbose output is:
1. Generated by `VerboseLogger` calling Rich console methods directly
2. Printed to stdout during live execution
3. **Not persisted** - only policy iterations and session metadata are saved

This means:
- Experiments cannot be replayed with identical verbose output
- No run ID is displayed in output
- No way to list past experiment runs
- Debugging requires re-running experiments (expensive with LLM calls)

### Desired State

1. Every experiment run has a **unique run ID** (e.g., `exp1-20251209-143022-a1b2c3`)
2. Run ID is printed at the start of experiment output
3. All verbose output events are persisted to the database as self-contained records
4. `castro replay <RUN_ID>` produces identical output using the StateProvider pattern
5. `castro results` lists all runs with key metrics

---

## Design Principles

### The StateProvider Pattern

Following the Payment Simulator pattern, we use Protocol-based abstraction:

```
                      ┌─────────────────────────────────────┐
                      │  display_experiment_output()        │
                      │  (Single Source of Truth)           │
                      └─────────────────┬───────────────────┘
                                        │
                    ┌───────────────────┴───────────────────┐
                    │    ExperimentStateProvider Protocol   │
                    └───────────────────┬───────────────────┘
                                        │
            ┌───────────────────────────┼───────────────────────────┐
            │                           │                           │
            ▼                           ▼                           ▼
┌───────────────────────┐  ┌────────────────────────┐  ┌──────────────────────┐
│ LiveExperimentProvider│  │ DatabaseExperimentProvider│ │ (Future providers)   │
│ (wraps ExperimentRunner)│ │ (wraps DB queries)        │ │                      │
└───────────────────────┘  └────────────────────────┘  └──────────────────────┘
```

**Key Invariants:**
- Display functions receive events from `ExperimentStateProvider`, never directly from runner or DB
- Events are **self-contained** - contain ALL data needed for display (no joins required)
- Both `run` and `replay` use the **same display function**
- Adding new event types follows a strict workflow

### Event-First Design

All verbose output is driven by **events**. An event contains:
- `event_type`: String identifier (e.g., `"iteration_start"`, `"monte_carlo_evaluation"`)
- `tick` or `iteration`: Temporal ordering key
- All display data pre-computed and included

Events are persisted as JSON in a new `experiment_events` table.

---

## Implementation Plan

### Phase 1: Run ID and Infrastructure

**Goal**: Add run ID generation and display infrastructure.

#### 1.1 Create Run ID Generator

**File**: `castro/run_id.py`

```python
"""Run ID generation for Castro experiments."""

import secrets
from datetime import datetime

def generate_run_id(experiment_name: str) -> str:
    """Generate a unique run ID.

    Format: {exp_name}-{YYYYMMDD}-{HHMMSS}-{random_hex}
    Example: exp1-20251209-143022-a1b2c3

    Args:
        experiment_name: Experiment name (exp1, exp2, exp3)

    Returns:
        Unique run ID string
    """
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    random_suffix = secrets.token_hex(3)  # 6 hex chars
    return f"{experiment_name}-{timestamp}-{random_suffix}"
```

#### 1.2 Update `ExperimentRunner` to Track Run ID

**File**: `castro/runner.py`

- Add `run_id` attribute to `ExperimentRunner`
- Generate run ID at start of `run()`
- Pass run ID to all event persistence calls
- Return run ID in `ExperimentResult`

#### 1.3 Update CLI to Display Run ID

**File**: `cli.py`

- Print run ID at start of experiment:
  ```
  Running exp1...
  Run ID: exp1-20251209-143022-a1b2c3
  ```
- Print run ID in final results table

**Tests (TDD)**:
- `test_run_id_format`: Validate run ID format matches specification
- `test_run_id_uniqueness`: Multiple calls generate unique IDs
- `test_run_id_in_output`: Run ID appears in experiment output

---

### Phase 2: Event Model and Persistence

**Goal**: Define event types and persist them to database.

#### 2.1 Define Experiment Event Types

**File**: `castro/events.py`

```python
"""Event types for Castro experiment replay identity."""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Any
from datetime import datetime


@dataclass
class ExperimentEvent:
    """Base event for experiment replay.

    All events are self-contained - they include ALL data needed
    for display, not just references to be looked up later.
    """
    event_type: str
    run_id: str
    iteration: int
    timestamp: datetime
    details: dict[str, Any] = field(default_factory=dict)


# Event type constants
EVENT_EXPERIMENT_START = "experiment_start"
EVENT_ITERATION_START = "iteration_start"
EVENT_MONTE_CARLO_EVALUATION = "monte_carlo_evaluation"
EVENT_LLM_CALL = "llm_call"
EVENT_POLICY_CHANGE = "policy_change"
EVENT_POLICY_REJECTED = "policy_rejected"
EVENT_CONVERGENCE_CHECK = "convergence_check"
EVENT_EXPERIMENT_END = "experiment_end"
```

#### 2.2 Create Events Table Schema

**File**: `castro/persistence/schema.py`

Add to `GameRepository.initialize_schema()`:

```sql
CREATE TABLE IF NOT EXISTS experiment_events (
    event_id VARCHAR PRIMARY KEY,
    run_id VARCHAR NOT NULL,
    event_type VARCHAR NOT NULL,
    iteration INTEGER NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    details JSON NOT NULL,

    CONSTRAINT fk_run_id FOREIGN KEY (run_id)
        REFERENCES experiment_runs(run_id)
);

CREATE INDEX IF NOT EXISTS idx_events_run_id ON experiment_events(run_id);
CREATE INDEX IF NOT EXISTS idx_events_type ON experiment_events(run_id, event_type);
CREATE INDEX IF NOT EXISTS idx_events_iteration ON experiment_events(run_id, iteration);
```

Also create `experiment_runs` table to store run metadata:

```sql
CREATE TABLE IF NOT EXISTS experiment_runs (
    run_id VARCHAR PRIMARY KEY,
    experiment_name VARCHAR NOT NULL,
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    status VARCHAR NOT NULL,
    final_cost INTEGER,
    best_cost INTEGER,
    num_iterations INTEGER,
    converged BOOLEAN,
    convergence_reason VARCHAR,
    model VARCHAR,
    master_seed BIGINT,
    config_json JSON
);

CREATE INDEX IF NOT EXISTS idx_runs_experiment ON experiment_runs(experiment_name);
CREATE INDEX IF NOT EXISTS idx_runs_started_at ON experiment_runs(started_at);
```

#### 2.3 Implement Event Persistence

**File**: `castro/persistence/event_writer.py`

```python
"""Write experiment events to database."""

class ExperimentEventWriter:
    """Writes experiment events to database for replay."""

    def __init__(self, conn: DuckDBPyConnection, run_id: str) -> None:
        self._conn = conn
        self._run_id = run_id

    def write_event(self, event: ExperimentEvent) -> None:
        """Persist an event to the database."""
        ...

    def write_batch(self, events: list[ExperimentEvent]) -> None:
        """Persist multiple events efficiently."""
        ...
```

**Tests (TDD)**:
- `test_event_persistence_round_trip`: Write event, read back, verify identical
- `test_event_ordering`: Events maintain iteration order
- `test_event_details_serialization`: Complex nested dicts serialize correctly

---

### Phase 3: Event Capture During Run

**Goal**: Capture all verbose output as events during experiment execution.

#### 3.1 Create EventEmitter Interface

**File**: `castro/events.py`

```python
from typing import Protocol

class EventEmitter(Protocol):
    """Protocol for event emission during experiment."""

    def emit(self, event: ExperimentEvent) -> None:
        """Emit an event."""
        ...
```

#### 3.2 Create Concrete EventEmitter

**File**: `castro/event_capture.py`

```python
class LiveEventCapture:
    """Captures events during live experiment execution.

    Both displays to console AND persists to database.
    """

    def __init__(
        self,
        event_writer: ExperimentEventWriter,
        verbose_logger: VerboseLogger,
    ) -> None:
        self._writer = event_writer
        self._logger = verbose_logger

    def emit(self, event: ExperimentEvent) -> None:
        """Capture and display event."""
        # 1. Persist to database (for replay)
        self._writer.write_event(event)

        # 2. Display to console (for live output)
        self._display_event(event)

    def _display_event(self, event: ExperimentEvent) -> None:
        """Route event to appropriate VerboseLogger method."""
        if event.event_type == EVENT_MONTE_CARLO_EVALUATION:
            self._logger.log_monte_carlo_evaluation(...)
        elif event.event_type == EVENT_LLM_CALL:
            self._logger.log_llm_call(...)
        # ... etc
```

#### 3.3 Refactor ExperimentRunner to Use Events

**File**: `castro/runner.py`

Current code directly calls `VerboseLogger`:
```python
self._verbose_logger.log_monte_carlo_evaluation(seed_results, mean_cost, std_cost)
```

Refactor to emit events:
```python
event = ExperimentEvent(
    event_type=EVENT_MONTE_CARLO_EVALUATION,
    run_id=self._run_id,
    iteration=iteration,
    timestamp=datetime.now(),
    details={
        "seed_results": [asdict(r) for r in seed_results],
        "mean_cost": mean_cost,
        "std_cost": std_cost,
    },
)
self._event_emitter.emit(event)
```

**Key Changes**:
- Add `EventEmitter` parameter to `ExperimentRunner.__init__`
- Replace all `VerboseLogger.log_*` calls with `EventEmitter.emit()`
- Events contain ALL data needed for display

**Tests (TDD)**:
- `test_all_verbose_output_emits_events`: Every log call produces an event
- `test_event_contains_all_display_data`: Events are self-contained
- `test_run_persists_events`: Events appear in database after run

---

### Phase 4: StateProvider Pattern

**Goal**: Implement the StateProvider abstraction for replay identity.

#### 4.1 Define ExperimentStateProvider Protocol

**File**: `castro/state_provider.py`

```python
"""State provider protocol for experiment replay identity."""

from typing import Protocol, Iterator
from castro.events import ExperimentEvent


class ExperimentStateProvider(Protocol):
    """Protocol for accessing experiment state.

    Implemented by:
    - LiveExperimentProvider (during run, wraps EventEmitter)
    - DatabaseExperimentProvider (during replay, wraps DB queries)
    """

    def get_run_metadata(self) -> dict[str, Any]:
        """Get run metadata (run_id, experiment_name, model, etc.)."""
        ...

    def get_events_for_iteration(self, iteration: int) -> list[ExperimentEvent]:
        """Get all events for a specific iteration."""
        ...

    def get_all_events(self) -> Iterator[ExperimentEvent]:
        """Iterate through all events in order."""
        ...

    def get_final_result(self) -> dict[str, Any]:
        """Get final experiment result."""
        ...
```

#### 4.2 Implement LiveExperimentProvider

**File**: `castro/state_provider.py`

```python
class LiveExperimentProvider:
    """StateProvider wrapping live experiment execution."""

    def __init__(self, runner: ExperimentRunner) -> None:
        self._runner = runner
        self._events: list[ExperimentEvent] = []

    def capture_event(self, event: ExperimentEvent) -> None:
        """Capture event during execution."""
        self._events.append(event)

    def get_all_events(self) -> Iterator[ExperimentEvent]:
        return iter(self._events)

    # ... implement other methods
```

#### 4.3 Implement DatabaseExperimentProvider

**File**: `castro/state_provider.py`

```python
class DatabaseExperimentProvider:
    """StateProvider wrapping database queries for replay."""

    def __init__(
        self,
        conn: DuckDBPyConnection,
        run_id: str,
    ) -> None:
        self._conn = conn
        self._run_id = run_id

    def get_all_events(self) -> Iterator[ExperimentEvent]:
        """Query events from database."""
        query = """
            SELECT event_type, run_id, iteration, timestamp, details
            FROM experiment_events
            WHERE run_id = ?
            ORDER BY iteration, timestamp
        """
        for row in self._conn.execute(query, [self._run_id]).fetchall():
            yield ExperimentEvent(
                event_type=row[0],
                run_id=row[1],
                iteration=row[2],
                timestamp=row[3],
                details=json.loads(row[4]),
            )

    # ... implement other methods
```

**Tests (TDD)**:
- `test_both_providers_implement_protocol`: Verify Protocol compliance
- `test_live_provider_captures_events`: Events captured during run
- `test_database_provider_reads_events`: Events read from DB

---

### Phase 5: Unified Display Function

**Goal**: Create a single display function that works for both run and replay.

#### 5.1 Create Unified Display Module

**File**: `castro/display.py`

```python
"""Unified display functions for experiment output.

These functions are the SINGLE SOURCE OF TRUTH for all output.
Both run and replay use these same functions with different providers.
"""

from castro.state_provider import ExperimentStateProvider
from castro.events import (
    EVENT_EXPERIMENT_START,
    EVENT_ITERATION_START,
    EVENT_MONTE_CARLO_EVALUATION,
    EVENT_LLM_CALL,
    EVENT_POLICY_CHANGE,
    EVENT_POLICY_REJECTED,
    EVENT_EXPERIMENT_END,
)
from rich.console import Console


def display_experiment_output(
    provider: ExperimentStateProvider,
    console: Console | None = None,
) -> None:
    """Display experiment output from any provider.

    This is the SINGLE SOURCE OF TRUTH for experiment output.
    Both `castro run` and `castro replay` use this function.

    Args:
        provider: ExperimentStateProvider (live or database)
        console: Rich Console for output
    """
    console = console or Console()

    # Display header with run metadata
    metadata = provider.get_run_metadata()
    console.print(f"\n[bold cyan]Starting {metadata['experiment_name']}[/bold cyan]")
    console.print(f"  Run ID: {metadata['run_id']}")
    console.print(f"  Description: {metadata['description']}")
    # ... etc

    # Display events
    for event in provider.get_all_events():
        _display_event(event, console)

    # Display final results
    result = provider.get_final_result()
    _display_final_results(result, console)


def _display_event(event: ExperimentEvent, console: Console) -> None:
    """Display a single event."""
    if event.event_type == EVENT_ITERATION_START:
        _display_iteration_start(event, console)
    elif event.event_type == EVENT_MONTE_CARLO_EVALUATION:
        _display_monte_carlo_evaluation(event, console)
    elif event.event_type == EVENT_LLM_CALL:
        _display_llm_call(event, console)
    elif event.event_type == EVENT_POLICY_CHANGE:
        _display_policy_change(event, console)
    elif event.event_type == EVENT_POLICY_REJECTED:
        _display_policy_rejected(event, console)
    # ... etc


def _display_monte_carlo_evaluation(
    event: ExperimentEvent,
    console: Console,
) -> None:
    """Display Monte Carlo evaluation results.

    CRITICAL: All data comes from event.details, not computed.
    """
    details = event.details
    seed_results = details["seed_results"]
    mean_cost = details["mean_cost"]
    std_cost = details["std_cost"]

    # Build and display table (same as current VerboseLogger)
    console.print(f"\n[bold]Monte Carlo Evaluation ({len(seed_results)} samples):[/bold]")
    # ... table rendering
```

#### 5.2 Refactor CLI to Use Unified Display

**File**: `cli.py`

```python
@app.command()
def run(...) -> None:
    """Run a Castro experiment."""
    # ... setup

    # Create live provider and run experiment
    with DatabaseManager(db_path) as db:
        provider = LiveExperimentProvider(runner)
        event_capture = LiveEventCapture(
            event_writer=ExperimentEventWriter(db.conn, run_id),
            provider=provider,
        )
        runner.set_event_emitter(event_capture)

        # Run experiment (events are captured and persisted)
        result = asyncio.run(runner.run())

        # Display output using unified function
        from castro.display import display_experiment_output
        display_experiment_output(provider, console)
```

**Tests (TDD)**:
- `test_display_function_with_live_provider`: Works with live provider
- `test_display_function_with_database_provider`: Works with database provider
- `test_display_output_identical`: Same events produce identical output

---

### Phase 6: Replay Command

**Goal**: Implement `castro replay <RUN_ID>` command.

#### 6.1 Add Replay Command

**File**: `cli.py`

```python
@app.command()
def replay(
    run_id: Annotated[
        str,
        typer.Argument(help="Run ID to replay (e.g., exp1-20251209-143022-a1b2c3)"),
    ],
    verbose: Annotated[
        bool,
        typer.Option("--verbose", "-v", help="Enable all verbose output"),
    ] = False,
    # ... other verbose flags
) -> None:
    """Replay experiment output from a previous run.

    Examples:
        castro replay exp1-20251209-143022-a1b2c3
        castro replay exp1-20251209-143022-a1b2c3 --verbose
    """
    from castro.display import display_experiment_output
    from castro.state_provider import DatabaseExperimentProvider

    # Find the database file for this run
    db_path = _find_database_for_run(run_id)
    if not db_path:
        console.print(f"[red]Run not found: {run_id}[/red]")
        raise typer.Exit(1)

    # Create database provider
    with DatabaseManager(str(db_path)) as db:
        provider = DatabaseExperimentProvider(db.conn, run_id)

        # Verify run exists
        metadata = provider.get_run_metadata()
        if not metadata:
            console.print(f"[red]Run not found in database: {run_id}[/red]")
            raise typer.Exit(1)

        # Build verbose config
        verbose_config = VerboseConfig.from_flags(verbose=verbose, ...)

        # Display using SAME function as live run
        display_experiment_output(provider, console, verbose_config)


def _find_database_for_run(run_id: str) -> Path | None:
    """Find the database file containing a specific run.

    Run ID format: {exp_name}-{date}-{time}-{random}
    Database path: results/{exp_name}.db
    """
    # Extract experiment name from run_id
    parts = run_id.split("-")
    if not parts:
        return None

    exp_name = parts[0]
    db_path = Path("results") / f"{exp_name}.db"

    if db_path.exists():
        return db_path

    return None
```

**Tests (TDD)**:
- `test_replay_finds_database`: Locates correct database from run ID
- `test_replay_loads_events`: Events loaded correctly from database
- `test_replay_output_matches_run`: Replay produces identical output to run

---

### Phase 7: Results Command

**Goal**: Implement `castro results` command to list all experiment runs.

#### 7.1 Add Results Command

**File**: `cli.py`

```python
@app.command()
def results(
    experiment: Annotated[
        str | None,
        typer.Argument(help="Filter by experiment (exp1, exp2, exp3)"),
    ] = None,
    limit: Annotated[
        int,
        typer.Option("--limit", "-n", help="Maximum results to show"),
    ] = 20,
) -> None:
    """List experiment runs with metrics.

    Examples:
        castro results              # All experiments
        castro results exp1         # Only exp1 runs
        castro results --limit 10   # Last 10 runs
    """
    from castro.persistence.queries import list_experiment_runs

    # Find all database files
    results_dir = Path("results")
    if not results_dir.exists():
        console.print("[yellow]No results directory found[/yellow]")
        return

    db_files = list(results_dir.glob("*.db"))
    if not db_files:
        console.print("[yellow]No experiment databases found[/yellow]")
        return

    # Query runs from all databases
    all_runs = []
    for db_path in db_files:
        with DatabaseManager(str(db_path)) as db:
            runs = list_experiment_runs(db.conn, experiment_filter=experiment)
            all_runs.extend(runs)

    # Sort by started_at descending
    all_runs.sort(key=lambda r: r["started_at"], reverse=True)
    all_runs = all_runs[:limit]

    if not all_runs:
        console.print("[yellow]No experiment runs found[/yellow]")
        return

    # Display table
    table = Table(title="Experiment Runs")
    table.add_column("Run ID", style="cyan")
    table.add_column("Experiment")
    table.add_column("Started", style="dim")
    table.add_column("Status")
    table.add_column("Iterations", justify="right")
    table.add_column("Final Cost", justify="right")
    table.add_column("Model")

    for run in all_runs:
        status_style = "green" if run["status"] == "converged" else "yellow"
        cost_str = f"${run['final_cost'] / 100:.2f}" if run["final_cost"] else "-"

        table.add_row(
            run["run_id"],
            run["experiment_name"],
            run["started_at"].strftime("%Y-%m-%d %H:%M"),
            f"[{status_style}]{run['status']}[/{status_style}]",
            str(run["num_iterations"] or "-"),
            cost_str,
            run.get("model", "-"),
        )

    console.print(table)
```

**Tests (TDD)**:
- `test_results_lists_runs`: Shows runs from database
- `test_results_filters_by_experiment`: Filtering works
- `test_results_shows_metrics`: Metrics displayed correctly

---

### Phase 8: Replay Identity Testing

**Goal**: Comprehensive tests ensuring replay identity.

#### 8.1 Gold Standard Replay Identity Test

**File**: `tests/integration/test_replay_identity.py`

```python
"""Gold standard tests for replay identity.

CRITICAL: These tests verify that replay output is IDENTICAL to run output.
"""

import pytest
from io import StringIO
from castro.runner import ExperimentRunner
from castro.display import display_experiment_output
from castro.state_provider import LiveExperimentProvider, DatabaseExperimentProvider


def test_replay_identity_exp1():
    """Replay of exp1 produces identical output to run."""
    # Run experiment with captured output
    run_output = StringIO()
    run_console = Console(file=run_output, force_terminal=True, width=120)

    runner = create_exp1_runner()
    provider = LiveExperimentProvider(runner)
    # ... run experiment

    display_experiment_output(provider, run_console)
    run_text = run_output.getvalue()

    # Replay from database with captured output
    replay_output = StringIO()
    replay_console = Console(file=replay_output, force_terminal=True, width=120)

    db_provider = DatabaseExperimentProvider(conn, run_id)
    display_experiment_output(db_provider, replay_console)
    replay_text = replay_output.getvalue()

    # Compare (strip timing info if present)
    run_normalized = _normalize_output(run_text)
    replay_normalized = _normalize_output(replay_text)

    assert run_normalized == replay_normalized, (
        f"Replay output differs from run output!\n"
        f"Run:\n{run_normalized[:500]}...\n\n"
        f"Replay:\n{replay_normalized[:500]}..."
    )


def test_all_event_types_have_display_handler():
    """Every event type has a display handler."""
    from castro.events import EVENT_TYPES
    from castro.display import _display_event

    for event_type in EVENT_TYPES:
        # Create minimal event
        event = ExperimentEvent(
            event_type=event_type,
            run_id="test-run",
            iteration=1,
            timestamp=datetime.now(),
            details={},  # Minimal details
        )

        # Should not raise
        try:
            output = StringIO()
            console = Console(file=output, force_terminal=True)
            _display_event(event, console)
        except KeyError as e:
            pytest.fail(f"Event type '{event_type}' missing display handler: {e}")


def test_event_details_complete():
    """Events contain all required details for display."""
    # Run experiment and capture events
    runner = create_test_runner()
    events = run_and_capture_events(runner)

    for event in events:
        if event.event_type == EVENT_MONTE_CARLO_EVALUATION:
            # Verify all required fields present
            assert "seed_results" in event.details
            assert "mean_cost" in event.details
            assert "std_cost" in event.details

            # Verify seed_results have all fields
            for result in event.details["seed_results"]:
                assert "seed" in result
                assert "cost" in result
                assert "settled" in result
                assert "total" in result
                assert "settlement_rate" in result

        elif event.event_type == EVENT_POLICY_CHANGE:
            assert "agent_id" in event.details
            assert "old_policy" in event.details
            assert "new_policy" in event.details
            assert "old_cost" in event.details
            assert "new_cost" in event.details
            assert "accepted" in event.details

        # ... verify other event types
```

---

## Anti-Patterns to Avoid

### 1. Never Display Without Events

```python
# BAD: Direct logging bypasses replay
console.print(f"Cost improved: {cost}")

# GOOD: Emit event, display function handles it
event = ExperimentEvent(
    event_type=EVENT_COST_IMPROVEMENT,
    details={"cost": cost},
    ...
)
emitter.emit(event)
```

### 2. Never Compute Display Data in Display Function

```python
# BAD: Recomputing in display (may differ in replay!)
def _display_monte_carlo(event, console):
    results = event.details["seed_results"]
    mean = sum(r["cost"] for r in results) // len(results)  # RECOMPUTED!
    console.print(f"Mean: {mean}")

# GOOD: Use pre-computed value from event
def _display_monte_carlo(event, console):
    mean = event.details["mean_cost"]  # FROM EVENT
    console.print(f"Mean: {mean}")
```

### 3. Never Query Additional Data During Display

```python
# BAD: Joining data during display
def _display_policy_change(event, console):
    agent_id = event.details["agent_id"]
    agent_config = query_agent_config(agent_id)  # QUERY!
    console.print(f"Agent capacity: {agent_config['capacity']}")

# GOOD: Include all needed data in event
def _display_policy_change(event, console):
    agent_capacity = event.details["agent_capacity"]  # FROM EVENT
    console.print(f"Agent capacity: {agent_capacity}")
```

---

## File Summary

| File | Purpose |
|------|---------|
| `castro/run_id.py` | Run ID generation |
| `castro/events.py` | Event type definitions |
| `castro/event_capture.py` | Event capture during live run |
| `castro/state_provider.py` | StateProvider protocol + implementations |
| `castro/display.py` | Unified display functions |
| `castro/persistence/event_writer.py` | Event persistence to database |
| `castro/persistence/queries.py` | Event queries for replay |
| `cli.py` | Updated with `replay` and `results` commands |
| `tests/integration/test_replay_identity.py` | Gold standard replay tests |

---

## Testing Checklist

When implementing each phase:

- [ ] Unit tests for new functions/classes
- [ ] Integration test for phase functionality
- [ ] Replay identity test (compare run vs replay output)
- [ ] All existing tests still pass
- [ ] mypy passes with no errors
- [ ] ruff check passes

---

## Dependencies

### Required Changes to Existing Files

1. **`runner.py`**:
   - Add `run_id` attribute
   - Add `EventEmitter` parameter
   - Replace all `VerboseLogger.log_*` calls with event emissions

2. **`cli.py`**:
   - Add `replay` command
   - Add `results` command
   - Update `run` command to display run ID

3. **`GameRepository` (in ai_cash_mgmt)**:
   - Add `experiment_runs` table schema
   - Add `experiment_events` table schema

4. **`verbose_logging.py`**:
   - May be deprecated or refactored to use events

---

## Success Criteria

1. `castro run exp1 --verbose` prints run ID at start
2. `castro results` shows all runs with metrics
3. `castro replay <RUN_ID> --verbose` produces **identical** output to original run
4. Replay identity is verified by automated tests
5. All tests pass (unit, integration, replay identity)
6. mypy and ruff checks pass

---

*Created: 2025-12-09*
