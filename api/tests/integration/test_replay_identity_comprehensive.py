"""Comprehensive replay identity tests.

This module tests that replay output is byte-for-byte identical to run output,
addressing all discrepancies documented in docs/plans/breaking_replay_identity.md.

Test Strategy:
1. Run a simulation with --persist --full-replay
2. Capture run output (JSON and verbose)
3. Replay the simulation
4. Capture replay output (JSON and verbose)
5. Assert exact matches for all metrics, events, and displays

Each test corresponds to a specific discrepancy from the breaking_replay_identity.md catalog.
"""

import json
import subprocess
import tempfile
from pathlib import Path
import pytest


class SimulationRunner:
    """Helper class to run and replay simulations, capturing outputs."""

    def __init__(self, config_path: str, db_path: str):
        """Initialize with config and database paths."""
        self.config_path = config_path
        self.db_path = db_path

    def run_simulation(self, verbose: bool = False) -> dict:
        """Run simulation and return parsed outputs.

        Returns:
            Dict with keys:
            - json: Parsed JSON output
            - stdout: Full stdout text
            - stderr: Full stderr text
        """
        cmd = [
            "uv", "run", "payment-sim", "run",
            "--config", self.config_path,
            "--persist",
            "--db-path", self.db_path,
            "--full-replay",
        ]
        if verbose:
            cmd.append("--verbose")

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=Path(__file__).parent.parent.parent,
        )

        if result.returncode != 0:
            raise RuntimeError(f"Run failed: {result.stderr}")

        # Extract JSON from output (last JSON block)
        json_output = self._extract_json(result.stdout)

        return {
            "json": json_output,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "returncode": result.returncode,
        }

    def replay_simulation(
        self,
        simulation_id: str,
        from_tick: int = 0,
        to_tick: int | None = None,
        verbose: bool = False
    ) -> dict:
        """Replay simulation and return parsed outputs.

        Returns:
            Dict with keys:
            - json: Parsed JSON output
            - stdout: Full stdout text
            - stderr: Full stderr text
        """
        cmd = [
            "uv", "run", "payment-sim", "replay",
            "--simulation-id", simulation_id,
            "--db-path", self.db_path,
            "--from-tick", str(from_tick),
        ]
        if to_tick is not None:
            cmd.extend(["--to-tick", str(to_tick)])
        if verbose:
            cmd.append("--verbose")

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=Path(__file__).parent.parent.parent,
        )

        if result.returncode != 0:
            raise RuntimeError(f"Replay failed: {result.stderr}")

        # Extract JSON from output
        json_output = self._extract_json(result.stdout)

        return {
            "json": json_output,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "returncode": result.returncode,
        }

    def _extract_json(self, stdout: str) -> dict:
        """Extract JSON object from stdout (finds last JSON block).

        Handles both single-line compact JSON and multi-line pretty-printed JSON.
        """
        lines = stdout.strip().split("\n")
        json_lines = []
        in_json = False

        for line in lines:
            if line.strip().startswith("{"):
                in_json = True
                json_lines = [line]
                # Check if JSON is on single line (compact format)
                if line.strip().endswith("}"):
                    try:
                        return json.loads(line)
                    except json.JSONDecodeError:
                        # Not valid JSON, continue multi-line parsing
                        pass
            elif in_json:
                json_lines.append(line)
                if line.strip().startswith("}"):
                    # Try to parse multi-line JSON
                    try:
                        return json.loads("\n".join(json_lines))
                    except json.JSONDecodeError:
                        # Not complete yet, continue
                        pass

        raise ValueError("No valid JSON found in output")


@pytest.fixture
def temp_db():
    """Create temporary database path for testing."""
    # Use TemporaryDirectory to get a clean path without creating the file
    # DuckDB will create the file when the simulation runs
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = str(Path(tmpdir) / "test.db")
        yield db_path
        # Cleanup happens automatically when tmpdir context exits


@pytest.fixture
def test_config():
    """Use existing example configuration."""
    # Use the advanced policy crisis config which is known to have good coverage
    config_path = Path(__file__).parent.parent.parent.parent / "examples" / "configs" / "advanced_policy_crisis.yaml"
    if not config_path.exists():
        # Fallback to simple example if advanced doesn't exist
        config_path = Path(__file__).parent.parent.parent.parent / "examples" / "configs" / "sim_config_simple_example.yaml"

    assert config_path.exists(), f"Config file not found: {config_path}"
    return str(config_path)


# ============================================================================
# DISCREPANCY #1: Transaction Settlement Counts
# ============================================================================

def test_replay_settlement_count_matches_run(test_config, temp_db):
    """Replay settlement count must match run settlement count exactly.

    Addresses:
    - Discrepancy 1.1: Per-tick settlement header differs (15 vs 8)
    - Discrepancy 1.2: Tick summary differs (10 vs 15)
    - Internal inconsistency: Run header vs summary (15 vs 10)
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run simulation
    run_output = runner.run_simulation(verbose=False)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]

    # Replay full simulation
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        verbose=False
    )

    # CRITICAL: Total settlements must match
    assert (
        replay_output["json"]["metrics"]["total_settlements"] ==
        run_output["json"]["metrics"]["total_settlements"]
    ), (
        f"Settlement count mismatch: "
        f"run={run_output['json']['metrics']['total_settlements']}, "
        f"replay={replay_output['json']['metrics']['total_settlements']}"
    )

    # Settlement rate must match
    assert (
        replay_output["json"]["metrics"]["settlement_rate"] ==
        run_output["json"]["metrics"]["settlement_rate"]
    ), (
        f"Settlement rate mismatch: "
        f"run={run_output['json']['metrics']['settlement_rate']}, "
        f"replay={replay_output['json']['metrics']['settlement_rate']}"
    )


def test_replay_single_tick_shows_full_simulation_stats(test_config, temp_db):
    """Replaying single tick must show full simulation statistics, not just that tick.

    Addresses:
    - Discrepancy 7.2: Metrics show only 6 arrivals instead of full simulation
    - Root cause: Replay confuses display range with simulation statistics
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run simulation
    run_output = runner.run_simulation(verbose=False)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]
    total_ticks = run_output["json"]["simulation"]["ticks_executed"]

    # Replay ONLY the last tick
    last_tick = total_ticks - 1
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        from_tick=last_tick,
        to_tick=last_tick,
        verbose=False
    )

    # CRITICAL: Even when replaying 1 tick, stats must show FULL simulation
    assert (
        replay_output["json"]["metrics"]["total_arrivals"] ==
        run_output["json"]["metrics"]["total_arrivals"]
    ), (
        f"Arrivals mismatch when replaying single tick: "
        f"run={run_output['json']['metrics']['total_arrivals']}, "
        f"replay={replay_output['json']['metrics']['total_arrivals']} "
        f"(replay should show full simulation, not just tick {last_tick})"
    )

    assert (
        replay_output["json"]["metrics"]["total_settlements"] ==
        run_output["json"]["metrics"]["total_settlements"]
    ), (
        f"Settlements mismatch when replaying single tick: "
        f"run={run_output['json']['metrics']['total_settlements']}, "
        f"replay={replay_output['json']['metrics']['total_settlements']}"
    )


# ============================================================================
# DISCREPANCY #2: LSM Cycle Counts
# ============================================================================

def test_replay_lsm_count_matches_run(test_config, temp_db):
    """Replay LSM cycle count must match run count.

    Addresses:
    - Discrepancy 1.2: Tick summary shows 3 LSM in run vs 1 in replay
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run simulation
    run_output = runner.run_simulation(verbose=False)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]

    # Replay
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        verbose=False
    )

    # LSM releases must match
    assert (
        replay_output["json"]["metrics"]["total_lsm_releases"] ==
        run_output["json"]["metrics"]["total_lsm_releases"]
    ), (
        f"LSM count mismatch: "
        f"run={run_output['json']['metrics']['total_lsm_releases']}, "
        f"replay={replay_output['json']['metrics']['total_lsm_releases']}"
    )


# ============================================================================
# DISCREPANCY #6: Agent Queue States
# ============================================================================

def test_replay_queue_sizes_match_run(test_config, temp_db):
    """Replay queue sizes must match run queue sizes.

    Addresses:
    - Discrepancy 6.1: CORRESPONDENT_HUB Queue 2 (16 vs 0)
    - Discrepancy 6.2: REGIONAL_TRUST Queue 2 (15 vs 0)
    - Discrepancy 7.3: Agents array queue1_size (38/41 vs 0)
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run simulation
    run_output = runner.run_simulation(verbose=False)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]

    # Replay
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        verbose=False
    )

    # Compare agent queue sizes
    run_agents = {agent["id"]: agent for agent in run_output["json"]["agents"]}
    replay_agents = {agent["id"]: agent for agent in replay_output["json"]["agents"]}

    for agent_id in run_agents:
        run_queue1 = run_agents[agent_id].get("queue1_size", 0)
        replay_queue1 = replay_agents[agent_id].get("queue1_size", 0)

        assert run_queue1 == replay_queue1, (
            f"Agent {agent_id} Queue1 mismatch: "
            f"run={run_queue1}, replay={replay_queue1}"
        )


# ============================================================================
# DISCREPANCY #7: JSON Output - Costs
# ============================================================================

def test_replay_total_costs_match_run(test_config, temp_db):
    """Replay total costs must match run costs.

    Addresses:
    - Discrepancy 7.4: Costs object (63899349 vs 0)
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run simulation
    run_output = runner.run_simulation(verbose=False)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]

    # Replay
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        verbose=False
    )

    # Total costs must match
    assert (
        replay_output["json"]["costs"]["total_cost"] ==
        run_output["json"]["costs"]["total_cost"]
    ), (
        f"Total costs mismatch: "
        f"run={run_output['json']['costs']['total_cost']}, "
        f"replay={replay_output['json']['costs']['total_cost']}"
    )


# ============================================================================
# VERBOSE OUTPUT TESTS
# ============================================================================

def test_replay_verbose_matches_run_verbose(test_config, temp_db):
    """Replay verbose output must be byte-for-byte identical to run (modulo timing).

    This is the GOLD STANDARD test - if this passes, replay identity is restored.

    Addresses: All discrepancies in verbose output format.
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run simulation with verbose
    run_output = runner.run_simulation(verbose=True)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]

    # Replay with verbose
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        verbose=True
    )

    # Normalize outputs (remove timing lines and JSON summary)
    # JSON summary line is excluded because:
    # 1. It contains timing info (duration_seconds, ticks_per_second) that always differs
    # 2. It contains path format differences (full path vs basename)
    # 3. Individual JSON metrics are tested separately by dedicated tests:
    #    - test_replay_settlement_count_matches_run
    #    - test_replay_lsm_count_matches_run
    #    - test_replay_queue_sizes_match_run
    #    - test_replay_total_costs_match_run
    def should_exclude_line(line: str) -> bool:
        """Check if line should be excluded from comparison."""
        # Timing-related lines
        timing_keywords = [
            "Duration:", "ticks/s", "in ", "Persisted", "Simulation complete",
            "Loading", "Replay complete", "Replayed from", "database replay mode",
            "Loading transaction data", "Loaded ", "Full replay data",
            "Note: Policy", "run with --persist"
        ]
        if any(keyword in line for keyword in timing_keywords):
            return True
        # JSON summary line (contains timing and path info that always differs)
        # JSON metrics are tested separately by dedicated tests
        stripped = line.strip()
        if stripped.startswith("{") and stripped.endswith("}"):
            return True
        return False

    run_lines = [
        line for line in run_output["stdout"].split("\n")
        if not should_exclude_line(line)
    ]

    replay_lines = [
        line for line in replay_output["stdout"].split("\n")
        if not should_exclude_line(line)
    ]

    # Compare line by line for better diagnostics
    max_lines = max(len(run_lines), len(replay_lines))
    mismatches = []

    for i in range(max_lines):
        run_line = run_lines[i] if i < len(run_lines) else "<MISSING>"
        replay_line = replay_lines[i] if i < len(replay_lines) else "<MISSING>"

        if run_line != replay_line:
            mismatches.append({
                "line_num": i + 1,
                "run": run_line,
                "replay": replay_line,
            })

    if mismatches:
        # Show first 10 mismatches for diagnosis
        error_msg = "Verbose output mismatch (first 10 differences):\n"
        for mismatch in mismatches[:10]:
            error_msg += f"\nLine {mismatch['line_num']}:\n"
            error_msg += f"  RUN:    {mismatch['run']}\n"
            error_msg += f"  REPLAY: {mismatch['replay']}\n"

        pytest.fail(error_msg)


# ============================================================================
# SPECIFIC DISCREPANCY TESTS (For Granular Debugging)
# ============================================================================

def test_replay_shows_queued_in_rtgs_events(test_config, temp_db):
    """Replay must show 'queued in RTGS' blocks.

    Addresses:
    - Discrepancy 2.1: Missing "ðŸ“‹ queued in RTGS" section
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run with verbose
    run_output = runner.run_simulation(verbose=True)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]

    # Replay with verbose
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        verbose=True
    )

    # Check for queued in RTGS blocks
    run_has_queued = "queued in RTGS" in run_output["stdout"]
    replay_has_queued = "queued in RTGS" in replay_output["stdout"]

    if run_has_queued:
        assert replay_has_queued, (
            "Run shows 'queued in RTGS' events but replay does not. "
            "Replay must display all event types that run displays."
        )


def test_replay_shows_cost_accrual_summary(test_config, temp_db):
    """Replay must show 'Costs Accrued This Tick' summary blocks.

    Addresses:
    - Discrepancy 4.1: Missing "ðŸ’° Costs Accrued This Tick" block
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run with verbose
    run_output = runner.run_simulation(verbose=True)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]

    # Replay with verbose
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        verbose=True
    )

    # Check for cost accrual blocks
    run_has_costs = "Costs Accrued This Tick" in run_output["stdout"]
    replay_has_costs = "Costs Accrued This Tick" in replay_output["stdout"]

    if run_has_costs:
        assert replay_has_costs, (
            "Run shows 'Costs Accrued This Tick' but replay does not. "
            "Replay must display all event types that run displays."
        )


def test_replay_eod_unsettled_count_matches(test_config, temp_db):
    """Replay end-of-day unsettled count must match run.

    Addresses:
    - Discrepancy 5.1: End-of-day banner (110 unsettled vs 0)
    """
    runner = SimulationRunner(test_config, temp_db)

    # Run with verbose
    run_output = runner.run_simulation(verbose=True)
    simulation_id = run_output["json"]["simulation"]["simulation_id"]

    # Replay with verbose
    replay_output = runner.replay_simulation(
        simulation_id=simulation_id,
        verbose=True
    )

    # Parse EOD lines
    def extract_eod_unsettled(stdout: str) -> list[int]:
        """Extract unsettled counts from EOD banners."""
        import re
        pattern = r"End of Day \d+ - (\d+) unsettled"
        matches = re.findall(pattern, stdout)
        return [int(m) for m in matches]

    run_unsettled = extract_eod_unsettled(run_output["stdout"])
    replay_unsettled = extract_eod_unsettled(replay_output["stdout"])

    assert run_unsettled == replay_unsettled, (
        f"EOD unsettled counts mismatch: "
        f"run={run_unsettled}, replay={replay_unsettled}"
    )


# ============================================================================
# EDGE CASE TESTS
# ============================================================================

def test_replay_empty_tick_range():
    """Replay with empty tick range should show full sim stats but no verbose output."""
    # This tests boundary conditions
    pass


def test_replay_with_no_full_replay_flag():
    """Replay of simulation without --full-replay should gracefully degrade."""
    # Should still show basic stats correctly even without tick agent states
    pass


def test_replay_interrupted_simulation():
    """Replay of interrupted simulation should work on available data."""
    # Test resilience to incomplete simulations
    pass


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
